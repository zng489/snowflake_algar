{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acd3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e81c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"snowflake-connector-python[pandas]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19fb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: pyarrow in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (21.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/asus/miniconda3/envs/ambiente/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd721cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a574de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fc067",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='ZHANG',\n",
    "    password='',\n",
    "    account='sjnsudy-xha96924',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='SNOWFLAKE_LEARNING_D',\n",
    "    schema='PUBLIC'\n",
    "    )\n",
    "\n",
    "# Criar cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Executar SELECT e converter em pandas DataFrame\n",
    "try:\n",
    "    cur.execute(\"SELECT * FROM SNOWFLAKE_LEARNING_DB.PUBLIC.TCU\")\n",
    "    df = cur.fetch_pandas_all()\n",
    "\n",
    "    # Exibir os primeiros registros\n",
    "    # print(df.head())\n",
    "    df.head()\n",
    "finally:\n",
    "    # Fechar cursor e conex√£o\n",
    "    cur.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f45cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas\n",
    "import pyarrow\n",
    "\n",
    "print(\"pandas:\", pandas.__version__)\n",
    "print(\"pyarrow:\", pyarrow.__version__)\n",
    "\n",
    "üöÄ Depois disso, seu c√≥digo com fetch_pandas_all() funcionar√° normalmente:\n",
    "\n",
    "df = cur.fetch_pandas_all()\n",
    "print(df.head())\n",
    "\n",
    "Se estiver usando Jupyter Notebook, reinicie o kernel ap√≥s instalar os pacotes para garantir que os m√≥dulos sejam carregados corretamente.\n",
    "\n",
    "Se quiser, posso ajudar voc√™ a montar um requirements.txt ou environment.yml para seu projeto. √â s√≥ pedir.\n",
    "You said:\n",
    "migracao de dados de databricks para snowflake\n",
    "ChatGPT said:\n",
    "\n",
    "Fazer migra√ß√£o de dados do Databricks para o Snowflake pode ser direto ou complexo, dependendo do volume de dados, formato dos arquivos e requisitos de seguran√ßa/performance.\n",
    "\n",
    "Abaixo te explico os principais m√©todos de migra√ß√£o, passo a passo, com ferramentas recomendadas, exemplos de c√≥digo e considera√ß√µes.\n",
    "‚úÖ Op√ß√µes de Migra√ß√£o de Dados de Databricks para Snowflake\n",
    "üîÅ 1. Via Snowflake Connector for Spark (m√©todo oficial e recomendado)\n",
    "\n",
    "O Databricks √© baseado em Spark, e a Snowflake fornece um conector nativo para Spark, que funciona perfeitamente no Databricks.\n",
    "üì¶ Instalar o conector\n",
    "\n",
    "Adicione ao seu cluster:\n",
    "\n",
    "    net.snowflake:spark-snowflake_2.12:2.9.0-spark_3.1\n",
    "\n",
    "    (ou vers√£o compat√≠vel com seu Spark)\n",
    "\n",
    "‚úÖ Exemplo de uso:\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\": \"your_account.snowflakecomputing.com\",\n",
    "  \"sfDatabase\": \"MEU_BANCO\",\n",
    "  \"sfSchema\": \"PUBLIC\",\n",
    "  \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "  \"sfRole\": \"ACCOUNTADMIN\",         # Opcional\n",
    "  \"sfUser\": \"SEU_USUARIO\",\n",
    "  \"sfPassword\": \"SUA_SENHA\"\n",
    "}\n",
    "\n",
    "# Exemplo: ler um DataFrame no Databricks\n",
    "df = spark.read.format(\"delta\").load(\"/mnt/datalake/meus_dados/clientes\")\n",
    "\n",
    "# Escrever para o Snowflake\n",
    "df.write \\\n",
    "  .format(\"snowflake\") \\\n",
    "  .options(**sfOptions) \\\n",
    "  .option(\"dbtable\", \"clientes_migrados\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "üìÅ 2. Via exporta√ß√£o de arquivos (Parquet/CSV) + Snowflake Stage + COPY INTO\n",
    "\n",
    "Se n√£o quiser usar Spark connector, pode:\n",
    "\n",
    "    Exportar dados do Databricks para arquivos (Parquet, CSV, etc.)\n",
    "\n",
    "    Carregar esses arquivos em um stage no Snowflake (p√∫blico, interno ou via S3)\n",
    "\n",
    "    Usar o COPY INTO no Snowflake para carregar na tabela.\n",
    "\n",
    "üîπ Exemplo:\n",
    "\n",
    "(1) Salvar como CSV ou Parquet:\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"/dbfs/tmp/clientes_migracao/\")\n",
    "\n",
    "(2) Mover arquivos para S3 (ou stage interno)\n",
    "\n",
    "(3) No Snowflake:\n",
    "\n",
    "CREATE STAGE stage_clientes;\n",
    "\n",
    "COPY INTO clientes_migrados\n",
    "FROM @stage_clientes\n",
    "FILE_FORMAT = (TYPE = PARQUET);\n",
    "\n",
    "üîÅ 3. Snowflake Native App for Databricks (pr√©via)\n",
    "\n",
    "A Snowflake tem integrado novos recursos com Databricks via Snowflake Native App Framework, mas isso ainda est√° em desenvolvimento e dispon√≠vel sob demanda.\n",
    "üîê Seguran√ßa\n",
    "\n",
    "    Evite hardcode de senha: use autentica√ß√£o OAuth, ou secrets do Databricks.\n",
    "\n",
    "    Prefira roles espec√≠ficas com permiss√µes limitadas.\n",
    "\n",
    "üìä Qual abordagem escolher?\n",
    "Cen√°rio\tAbordagem Ideal\n",
    "Migra√ß√£o de grandes volumes (~TB)\tSnowflake Spark Connector\n",
    "Integra√ß√£o cont√≠nua entre plataformas\tSpark Connector + pipelines\n",
    "Arquivos no Data Lake que v√£o para Snowflake\tCOPY INTO + S3 + Stage\n",
    "Baixo volume e uso pontual\tExporta√ß√£o manual\n",
    "‚ùìQuer um exemplo pr√°tico no seu contexto?\n",
    "\n",
    "Me diga:\n",
    "\n",
    "    O formato dos dados no Databricks (Delta, Parquet, CSV, tabela Spark‚Ä¶)\n",
    "\n",
    "    Onde voc√™ quer armazenar no Snowflake (nome da tabela, banco, schema)\n",
    "\n",
    "    Se h√° autentica√ß√£o especial (OAuth, tokens, etc.)\n",
    "\n",
    "    Se j√° usa AWS, Azure ou GCP\n",
    "\n",
    "Posso montar um script real com base nisso.\n",
    "\n",
    "\n",
    "ChatGPT can make mistakes. Check important info.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
